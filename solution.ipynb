{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook solution test technique \n",
    "### Amine MEKKI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be implementing a solution to detect phishing URLs. The notebook is structured as follows:\n",
    "\n",
    "- Data Analysis and Feature Engineering: This section involves initial data analysis and the creation of various features to help in phishing URL detection. Some of the features might take a considerable amount of time to compute.\n",
    "\n",
    "- Processed Data: To save time, I recommend skipping to the part where the processed data is loaded after importing the necessary libraries. This allows you to bypass the time-consuming feature engineering steps and directly proceed with model training and evaluation.\n",
    "- [Go to processed data](#Lets-load-the-processed-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import dns.resolver\n",
    "from collections import Counter\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length, col, regexp_replace, monotonically_increasing_id, udf, col, expr\n",
    "from pyspark.ml.feature import Tokenizer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, StructType, StructField\n",
    "from pyspark.ml.stat import Correlation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.sql import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('PhishingURLDetection') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.storage.memoryFraction\", \"0.6\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = './artifacts/data/raw/urls_dataset.csv'\n",
    "df_spark = spark.read.csv(file_path, sep=\"\\t\",header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.printSchema()\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spark = df_spark.withColumn('label', col('label').cast('integer'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spark = df_spark.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2 checking for missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values so that is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = df_spark.groupBy('label').count()\n",
    "class_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.filter((col('label') == 0) | (col('label') == 1))\n",
    "class_distribution = df_spark.groupBy('label').count()\n",
    "class_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_distribution = df_spark.groupBy('label').count().toPandas()\n",
    "plt.bar(class_distribution['label'], class_distribution['count'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 65% of non phishing urls \n",
    "and 35% of phishing urls  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following case i wanted to investigate the prefix, i saw that sometimes there are different prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the original dataframe\n",
    "df_spark_copy = df_spark\n",
    "\n",
    "def extract_prefix(url):\n",
    "    if url.startswith(\"http\"):\n",
    "        return \"http\"\n",
    "    elif url.startswith(\"https\"):\n",
    "        return \"https\"\n",
    "    elif url.startswith(\"jvvru\"):\n",
    "        return \"jvvru\"\n",
    "    elif url.startswith(\"jvvr\"):\n",
    "        return \"jvvr\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "extract_prefix_udf = udf(extract_prefix, StringType())\n",
    "df_spark_copy = df_spark_copy.withColumn(\"prefix\", extract_prefix_udf(col(\"url\")))\n",
    "df_spark_copy.show()\n",
    "contingency_table = df_spark_copy.groupBy(\"prefix\").pivot(\"label\").count().na.fill(0)\n",
    "contingency_table.show()\n",
    "df_grouped = df_spark_copy.groupBy(\"prefix\").agg(\n",
    "    expr(\"sum(label)\").alias(\"phishing_count\"),\n",
    "    expr(\"count(*)\").alias(\"total_count\"),\n",
    "    expr(\"sum(label) / count(*) * 100\").alias(\"phishing_percentage\")\n",
    ")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our urls are textual data. Let's try to use these textual data to extract some numerical data.\n",
    "The following features have been selected after carefully looking into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_words = ['secure', 'account', 'webscr', 'login', 'ebayisapi', 'signin']\n",
    "\n",
    "def subdomain_level(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the subdomain level of the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: The subdomain level\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        if hostname:\n",
    "            return len(hostname.split('.')) - 2\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in subdomain_level: {e}\")\n",
    "        return 0\n",
    "\n",
    "def path_level(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the path level of the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: The path level\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = urlparse(url).path\n",
    "        return len(path.split('/')) - 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error in path_level: {e}\")\n",
    "        return 0\n",
    "\n",
    "def count_dashes_hostname(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of dashes in the hostname of the URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: The number of dashes in the hostname\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        if hostname:\n",
    "            return hostname.count('-')\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_dashes_hostname: {e}\")\n",
    "        return 0\n",
    "\n",
    "def no_https(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the URL does not have HTTPS\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if the URL does not have HTTPS, 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 0 if urlparse(url).scheme == 'https' else 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error in no_https: {e}\")\n",
    "        return 0\n",
    "\n",
    "def random_string(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the URL has a random string of characters\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if the URL has a random string of characters, 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 1 if re.search(r'[a-zA-Z]{10,}', url) else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in random_string: {e}\")\n",
    "        return 0\n",
    "\n",
    "def ip_address(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the URL has an IP address\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if the URL has an IP address, 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 1 if re.match(r'^(http|https):\\/\\/\\d+\\.\\d+\\.\\d+\\.\\d+', url) else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ip_address: {e}\")\n",
    "        return 0\n",
    "\n",
    "def domain_in_subdomains(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the domain is in the subdomains of the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if the domain is in the subdomains, 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        if hostname:\n",
    "            domain_parts = hostname.split('.')\n",
    "            return 1 if len(domain_parts) > 2 else 0\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in domain_in_subdomains: {e}\")\n",
    "        return 0\n",
    "\n",
    "def domain_in_paths(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the domain is in the paths of the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if the domain is in the paths, 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = urlparse(url).path\n",
    "        hostname = urlparse(url).hostname\n",
    "        if hostname and path:\n",
    "            return 1 if hostname in path else 0\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in domain_in_paths: {e}\")\n",
    "        return 0\n",
    "\n",
    "def https_in_hostname(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the URL has HTTPS in the hostname\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if the URL has HTTPS in the hostname, 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        return 1 if hostname and 'https' in hostname else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in https_in_hostname: {e}\")\n",
    "        return 0\n",
    "\n",
    "def hostname_length(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the length of the hostname of the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: The length of the hostname\n",
    "    \"\"\"\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        return len(hostname) if hostname else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in hostname_length: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def count_sensitive_words(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of sensitive words in the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: The number of sensitive words in the URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return sum(word in url.lower() for word in sensitive_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_sensitive_words: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def count_www(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of 'www' in the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: The number of 'www' in the URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return url.count('www')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_www: {e}\")\n",
    "        return 0\n",
    "\n",
    "def fd_length(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the length of the first directory of the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: The length of the first directory\n",
    "    \"\"\"\n",
    "    urlpath= urlparse(url).path\n",
    "    try:\n",
    "        return len(urlpath.split('/')[1])\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def has_prefix_jvvr(url : str) -> int:\n",
    "    \"\"\"\n",
    "    Check if the URL has the prefix 'jvvr'\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        int: 1 if the URL has the prefix 'jvvr', 0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 1 if url.startswith(\"jvvr\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in has_prefix_jvvr: {e}\")\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def has_prefix_jvvru(url : str) -> int:\n",
    "    try:\n",
    "        return 1 if url.startswith(\"jvvru\") else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in has_prefix_jvvru: {e}\")\n",
    "        return 0\n",
    "\n",
    "def calculate_entropy(url : str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        float: The entropy of the URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        counter = Counter(url)\n",
    "        total_len = len(url)\n",
    "        entropy = -sum((count / total_len) * math.log2(count / total_len) for count in counter.values())\n",
    "        return entropy\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_entropy: {e}\")\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am defining the user defined functions for the above feature funnctions\n",
    "\n",
    "subdomain_level_udf = udf(subdomain_level, IntegerType())\n",
    "path_level_udf = udf(path_level, IntegerType())\n",
    "count_dashes_hostname_udf = udf(count_dashes_hostname, IntegerType())\n",
    "no_https_udf = udf(no_https, IntegerType())\n",
    "random_string_udf = udf(random_string, IntegerType())\n",
    "ip_address_udf = udf(ip_address, IntegerType())\n",
    "domain_in_subdomains_udf = udf(domain_in_subdomains, IntegerType())\n",
    "sensitive_words = ['secure', 'account', 'webscr', 'login', 'ebayisapi', 'signin']\n",
    "domain_in_paths_udf = udf(domain_in_paths, IntegerType())\n",
    "hostname_length_udf = udf(hostname_length, IntegerType())\n",
    "https_in_hostname_udf = udf(https_in_hostname, IntegerType())\n",
    "count_sensitive_words_udf = udf(count_sensitive_words, IntegerType())\n",
    "count_www_udf = udf(count_www, IntegerType())\n",
    "fd_length_udf = udf(fd_length, IntegerType())\n",
    "has_prefix_jvvr_udf = udf(has_prefix_jvvr, IntegerType())\n",
    "has_prefix_jvvru_udf = udf(has_prefix_jvvru, IntegerType())\n",
    "entropy_udf = udf(calculate_entropy, FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i am applying the above udfs ..\n",
    "df_spark = df_spark.withColumn('url_length', length(col('url')))\n",
    "df_spark = df_spark.withColumn('num_dots', length(regexp_replace(col('url'), '[^.]', '')) - 1)\n",
    "df_spark = df_spark.withColumn('num_hyphens', length(regexp_replace(col('url'), '[^-]', '')) - 1)\n",
    "df_spark = df_spark.withColumn('num_digits', length(regexp_replace(col('url'), '[^\\d]', '')))\n",
    "df_spark = df_spark.withColumn('num_alpha', length(regexp_replace(col('url'), '[^a-zA-Z]', '')))\n",
    "df_spark = df_spark.withColumn('subdomain_level', subdomain_level_udf('url'))\n",
    "df_spark = df_spark.withColumn('path_level', path_level_udf('url'))\n",
    "df_spark = df_spark.withColumn('count_dashes_hostname', count_dashes_hostname_udf('url'))\n",
    "df_spark = df_spark.withColumn('no_https', no_https_udf('url'))\n",
    "df_spark = df_spark.withColumn('random_string', random_string_udf('url'))\n",
    "df_spark = df_spark.withColumn('ip_address', ip_address_udf('url'))\n",
    "df_spark = df_spark.withColumn('domain_in_subdomains', domain_in_subdomains_udf('url'))\n",
    "df_spark = df_spark.withColumn('domain_in_paths', domain_in_paths_udf('url'))\n",
    "df_spark = df_spark.withColumn('https_in_hostname', https_in_hostname_udf('url'))\n",
    "df_spark = df_spark.withColumn('hostname_length', hostname_length_udf('url'))\n",
    "df_spark = df_spark.withColumn('count_sensitive_words', count_sensitive_words_udf('url'))\n",
    "df_spark = df_spark.withColumn('count_www', count_www_udf('url'))\n",
    "df_spark = df_spark.withColumn('fd_length', fd_length_udf(col('url')))\n",
    "df_spark = df_spark.withColumn(\"has_prefix_jvvr\", has_prefix_jvvr_udf(col(\"url\")))\n",
    "df_spark = df_spark.withColumn(\"has_prefix_jvvru\", has_prefix_jvvru_udf(col(\"url\")))\n",
    "df_spark = df_spark.withColumn('url_entropy', entropy_udf(col('url')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think also the amount of special characters can be an indicator or a url that is classified as phishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['@','?','-','=','.','#','%','+','$','!','*',',','//', '&']\n",
    "\n",
    "for a in features:\n",
    "    df_spark = df_spark.withColumn('num_'+a, length(regexp_replace(col('url'), '[^'+a+']', '')))\n",
    "\n",
    "df_spark = df_spark.withColumnRenamed('num_.', 'num_dot')\n",
    "df_spark = df_spark.withColumnRenamed('num_#', 'num_hash')\n",
    "df_spark = df_spark.withColumnRenamed('num_%', 'num_percent')\n",
    "df_spark = df_spark.withColumnRenamed('num_+', 'num_plus')\n",
    "df_spark = df_spark.withColumnRenamed('num_$', 'num_dollar')\n",
    "df_spark = df_spark.withColumnRenamed('num_!', 'num_exclamation')\n",
    "df_spark = df_spark.withColumnRenamed('num_*', 'num_asterisk')\n",
    "df_spark = df_spark.withColumnRenamed('num_,', 'num_comma')\n",
    "df_spark = df_spark.withColumnRenamed('num_&', 'num_query_symbol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Advanced features DNS Takes too much time. that's why i saved them \n",
    "After doing some research about URLs i found that the following DNS features can be helpful to detect if a url phishing. This helped to gain 10% in most metrics that i have used.\n",
    "\n",
    "These features they provide insights into the website's configuration and behavior patterns. Legitimate websites generally have well configured DNS settings with multiple A records for load balancing and fault tolerance, while phishing websites may not invest in such robust configurations, leading to fewer DNS records. \n",
    "\n",
    "So by analyzing DNS resolution patterns can reveal some anomalies typical of phishing sites, which they often use newly registered domains with minimal DNS history. Additionally, Using IP address analysis we can identify less reputable IP ranges associated with phishing sites, and features like the sum of IP octets can highlight these irregularities. \n",
    "\n",
    "Lastly, the URL structure and length can also indicate phishing attempts, as legitimate domains are typically concise and recognizable, whereas phishing domains may be excessively long or contain unusual subdomains to mislead users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_scheme(url : str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure that the URL has a scheme (http or https)\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        str: The URL with a scheme\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not urlparse(url).scheme:\n",
    "            return 'http://' + url\n",
    "        return url\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ensure_scheme: {e}\")\n",
    "        return url\n",
    "\n",
    "\n",
    "def get_dns_info(url : str) -> tuple:\n",
    "    \"\"\"\n",
    "    Get DNS information for the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the number of DNS answers, the number of IP octets, the sum of the IP octets, \n",
    "        whether the URL has an IP address, and the length of the domain\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = ensure_scheme(url)\n",
    "        domain = urlparse(url).hostname\n",
    "        print(f\"fetched domain: {domain}\")\n",
    "        if not domain:\n",
    "            return (0, 0, 0, 0, 0)\n",
    "\n",
    "        answers = dns.resolver.resolve(domain, 'A')\n",
    "        ip_address = answers[0].address if answers else None\n",
    "        return (len(answers), len(ip_address.split('.')), sum(int(x) for x in ip_address.split('.')), 1 if ip_address else 0, len(domain))\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving DNS data for {url}: {e}\")\n",
    "        return (0, 0, 0, 0, 0)  # default values\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"num_dns_answers\", IntegerType(), True),\n",
    "    StructField(\"num_ip_octets\", IntegerType(), True),\n",
    "    StructField(\"sum_ip_octets\", IntegerType(), True),\n",
    "    StructField(\"has_ip_address\", IntegerType(), True),\n",
    "    StructField(\"domain_length\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dns_info_udf = udf(get_dns_info, schema)\n",
    "\n",
    "df_spark = df_spark.withColumn('dns_info', get_dns_info_udf('url'))\n",
    "\n",
    "df_spark = df_spark.cache()\n",
    "df_spark.count()\n",
    "\n",
    "for field in schema.fields:\n",
    "    df_spark = df_spark.withColumn(field.name, df_spark['dns_info'][field.name])\n",
    "\n",
    "df_spark = df_spark.drop('dns_info')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.write.mode('overwrite').parquet('./artifacts/data/old_processed/urls_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spark = spark.read.parquet('./artifacts/data/old_processed/urls_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis of the built features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = df_spark.columns\n",
    "\n",
    "numeric_columns = [c for c, t in df_spark.dtypes if t in ['double', 'float']]\n",
    "non_numeric_columns = [c for c, t in df_spark.dtypes if t not in ['double', 'float']]\n",
    "\n",
    "null_exprs = [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in all_columns]\n",
    "\n",
    "nan_exprs = [F.count(F.when(F.isnan(c), c)).alias(c) for c in numeric_columns]\n",
    "\n",
    "exprs = null_exprs + nan_exprs\n",
    "\n",
    "df_spark.select(exprs).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# let's analyze the correlation between the features \n",
    "features = ['url_length', 'num_dots', 'num_hyphens', 'num_digits', 'num_alpha', 'subdomain_level', 'path_level', 'count_dashes_hostname', 'no_https', 'random_string', 'ip_address', 'domain_in_subdomains', 'domain_in_paths', 'https_in_hostname', 'hostname_length', 'count_sensitive_words', 'count_www', 'fd_length', 'has_prefix_jvvr', 'has_prefix_jvvru', 'url_entropy', 'num_@', 'num_?', 'num_-', 'num_=', 'num_dot', 'num_hash', 'num_percent', 'num_plus', 'num_dollar',\n",
    "    'num_exclamation', 'num_asterisk', 'num_comma', 'num_//', 'num_query_symbol', 'num_dns_answers', 'num_ip_octets', 'sum_ip_octets', 'has_ip_address', 'domain_length']\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "df_spark = assembler.transform(df_spark)\n",
    "\n",
    "correlation_matrix = Correlation.corr(df_spark, 'features').collect()[0][0]\n",
    "correlation_matrix_arr = correlation_matrix.toArray()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(correlation_matrix_arr, columns=features, index=features)\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df, annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_label = Correlation.corr(df_spark, 'features', 'pearson').head()[0].toArray()[-1]\n",
    "correlation_with_label = pd.Series(correlation_with_label, index=features)\n",
    "correlation_with_label = correlation_with_label.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "correlation_with_label.plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can the dns feature are highly correlated with the target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution and resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class distribution\n",
    "here i am investigating distribution of our target classes to understand the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.filter((col('label') == 0) | (col('label') == 1))\n",
    "class_distribution = df_spark.groupBy('label').count()\n",
    "class_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0 = df_spark.filter(df_spark.label == 0)\n",
    "df_class_1 = df_spark.filter(df_spark.label == 1)\n",
    "ratio = int(df_class_0.count() / df_class_1.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_count = df_spark.count()\n",
    "class_0_count = df_class_0.count()\n",
    "class_1_count = df_class_1.count()\n",
    "\n",
    "class_0_weight = total_count / (2 * class_0_count)\n",
    "class_1_weight = total_count / (2 * class_1_count)\n",
    "\n",
    "df_spark = df_spark.withColumn(\"class_weight\", F.when(df_spark.label == 0, class_0_weight).otherwise(class_1_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Class 0 weight: {class_0_weight}\")\n",
    "print(f\"Class 1 weight: {class_1_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### downsampling\n",
    "We reduce the number of instances in the majority class to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0_undersampled = df_class_0.sample(False, df_class_1.count() / df_class_0.count())\n",
    "df_balanced_down = df_class_0_undersampled.union(df_class_1)\n",
    "df_balanced_down = df_balanced_down.drop('class_weight')\n",
    "\n",
    "train_data_down, test_data_down = df_balanced_down.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_down = df_balanced_down.filter((col('label') == 0) | (col('label') == 1))\n",
    "class_distribution = df_balanced_down.groupBy('label').count()\n",
    "class_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overssampling\n",
    "Here i am using SMOTE, the Synthetic Minority Over-sampling Technique to generate synthetic instances for the minority class to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df_spark.toPandas()\n",
    "non_numeric_cols = df_pandas.select_dtypes(include=['object']).columns\n",
    "df_pandas = df_pandas.drop(columns=non_numeric_cols)\n",
    "\n",
    "X = df_pandas.drop('label', axis=1)\n",
    "y = df_pandas['label']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "df_resampled['label'] = y_resampled\n",
    "\n",
    "df_over_resampled_spark = spark.createDataFrame(df_resampled)\n",
    "df_over_resampled_spark.show()\n",
    "\n",
    "class_distribution_resampled = df_over_resampled_spark.groupBy('label').count()\n",
    "class_distribution_resampled.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "df_over_resampled_spark = assembler.transform(df_over_resampled_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_resampled_spark = df_over_resampled_spark.drop('class_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept_features_cols = [\n",
    "#             'url_length', 'num_dots', 'num_hyphens', 'num_digits', 'num_alpha', \n",
    "#             'subdomain_level', 'path_level', 'count_dashes_hostname', 'no_https', \n",
    "#             'random_string', 'ip_address', 'domain_in_subdomains', 'domain_in_paths', \n",
    "#             'https_in_hostname', 'hostname_length', 'count_sensitive_words', \n",
    "#             'url_entropy', 'num_@', 'num_?', 'num_-', 'num_=', 'num_dot', \n",
    "#             'num_hash', 'num_percent', 'num_plus', 'num_dollar', 'num_exclamation', \n",
    "#             'num_asterisk', 'num_comma', 'num_//', 'num_query_symbol', \n",
    "#             'num_dns_answers', 'num_ip_octets', 'sum_ip_octets', 'has_ip_address', \n",
    "#             'domain_length'\n",
    "#         ]\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=kept_features_cols, outputCol='kept_features_cols')\n",
    "# df_spark = assembler.transform(df_spark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_over, test_data_over = df_over_resampled_spark.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, i'll train and evaluate different machine learning models to detect phishing URLs. \n",
    "\n",
    "I am three models: \n",
    "- Random Forest, \n",
    "- Gradient Boosting\n",
    "- Logistic Regression. \n",
    "    \n",
    "Each model is evaluated using accuracy, precision, recall, F1 score, and AUC (Area Under the ROC Curve).\n",
    "\n",
    "\n",
    "#### Stacking : Meta-Classifier:\n",
    "I am using stacking approach where predictions from Random Forest, Logistic Regression, and Gradient Boosting are used as features to train a meta-classifier (Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(\n",
    "    train_data: DataFrame, \n",
    "    test_data: DataFrame, \n",
    "    feature_cols: str, \n",
    "    use_class_weights: bool = False\n",
    ") -> Tuple[Dict[str, Dict[str, float]], Dict[str, RandomForestClassifier]]:\n",
    "    \"\"\"\n",
    "    Train and evaluate machine learning models.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): Training data.\n",
    "        test_data (DataFrame): Testing data.\n",
    "        feature_cols (str): Name of the feature column.\n",
    "        use_class_weights (bool): Whether to use class weights.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, float]], Dict[str, RandomForestClassifier]]: Evaluation results and trained models.\n",
    "    \"\"\"\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction')\n",
    "    \n",
    "    if use_class_weights:\n",
    "        models = {\n",
    "            'RandomForest': RandomForestClassifier(featuresCol=feature_cols, labelCol='label', weightCol='class_weight'),\n",
    "            'GBT': GBTClassifier(featuresCol=feature_cols, labelCol='label', weightCol='class_weight'),\n",
    "            'LogisticRegression': LogisticRegression(featuresCol=feature_cols, labelCol='label', weightCol='class_weight')\n",
    "        }\n",
    "    else:\n",
    "        models = {\n",
    "            'RandomForest': RandomForestClassifier(featuresCol=feature_cols, labelCol='label'),\n",
    "            'GBT': GBTClassifier(featuresCol=feature_cols, labelCol='label'),\n",
    "            'LogisticRegression': LogisticRegression(featuresCol=feature_cols, labelCol='label')\n",
    "        }\n",
    "    \n",
    "    trained_models = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        trained_models[model_name] = model.fit(train_data)\n",
    "    \n",
    "    def evaluate_model(model, test_data, evaluator):\n",
    "        predictions = model.transform(test_data)\n",
    "        auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "        accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_data.count())\n",
    "        tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "        fp = predictions.filter((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "        fn = predictions.filter((predictions.label == 1) & (predictions.prediction == 0)).count()\n",
    "        \n",
    "        precision = tp / float(tp + fp) if (tp + fp) != 0 else 0.0\n",
    "        recall = tp / float(tp + fn) if (tp + fn) != 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0.0\n",
    "        \n",
    "        return accuracy, precision, recall, f1, auc\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in trained_models.items():\n",
    "        accuracy, precision, recall, f1, auc = evaluate_model(model, test_data, evaluator)\n",
    "        results[model_name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'AUC': auc\n",
    "        }\n",
    "    \n",
    "    return results, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_normal, trained_models_normal = train_and_evaluate_models(train_data, test_data, 'features', use_class_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, metrics in results_normal.items():\n",
    "    print(f'{model_name} - Accuracy: {metrics[\"Accuracy\"]}, Precision: {metrics[\"Precision\"]}, Recall: {metrics[\"Recall\"]}, F1: {metrics[\"F1\"]}, AUC: {metrics[\"AUC\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_resampled_over, trained_models_resampled_over = train_and_evaluate_models(train_data_over, test_data_over, \"features\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, metrics in results_resampled_over.items():\n",
    "    print(f'{model_name} - Accuracy: {metrics[\"Accuracy\"]}, Precision: {metrics[\"Precision\"]}, Recall: {metrics[\"Recall\"]}, F1: {metrics[\"F1\"]}, AUC: {metrics[\"AUC\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with Down Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_resampled_down, trained_models_resampled_down = train_and_evaluate_models(train_data_down, test_data_down, \"features\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, metrics in results_resampled_down.items():\n",
    "    print(f'{model_name} - Accuracy: {metrics[\"Accuracy\"]}, Precision: {metrics[\"Precision\"]}, Recall: {metrics[\"Recall\"]}, F1: {metrics[\"F1\"]}, AUC: {metrics[\"AUC\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Mpdel Training and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_stacking_model(\n",
    "    train_data: DataFrame, \n",
    "    test_data: DataFrame, \n",
    "    rf_model: RandomForestClassifier, \n",
    "    lr_model: LogisticRegression, \n",
    "    gbt_model: GBTClassifier, \n",
    "    feature_cols: str,\n",
    "    use_class_weights=False\n",
    ") -> Tuple[Dict[str, float], LogisticRegression]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a stacking meta-classifier.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): Training data.\n",
    "        test_data (DataFrame): Testing data.\n",
    "        rf_model (RandomForestClassifier): Pre-trained Random Forest model.\n",
    "        lr_model (LogisticRegression): Pre-trained Logistic Regression model.\n",
    "        gbt_model (GBTClassifier): Pre-trained Gradient Boosting Trees model.\n",
    "        feature_cols (str): Name of the feature column.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, float], LogisticRegression]: Evaluation results and the trained meta-classifier.\n",
    "    \"\"\"\n",
    "    train_data = train_data.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "    test_data = test_data.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "\n",
    "    train_rf_predictions = rf_model.transform(train_data).select('unique_id', col('prediction').alias('rf_prediction'))\n",
    "    train_lr_predictions = lr_model.transform(train_data).select('unique_id', col('prediction').alias('lr_prediction'))\n",
    "    train_gb_predictions = gbt_model.transform(train_data).select('unique_id', col('prediction').alias('gb_prediction'))\n",
    "\n",
    "    meta_train_data = train_data.join(train_rf_predictions, 'unique_id').join(train_lr_predictions, 'unique_id').join(train_gb_predictions, 'unique_id')\n",
    "\n",
    "    feature_cols = ['rf_prediction', 'lr_prediction', 'gb_prediction'] + [feature_cols]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='meta_features')\n",
    "    meta_train_data = assembler.transform(meta_train_data)\n",
    "\n",
    "    if use_class_weights:\n",
    "        meta_lr = LogisticRegression(labelCol='label', featuresCol='meta_features', weightCol='class_weight')\n",
    "    else:\n",
    "        meta_lr = LogisticRegression(labelCol='label', featuresCol='meta_features')\n",
    "    \n",
    "    meta_lr_model = meta_lr.fit(meta_train_data)\n",
    "\n",
    "    test_rf_predictions = rf_model.transform(test_data).select('unique_id', col('prediction').alias('rf_prediction'))\n",
    "    test_lr_predictions = lr_model.transform(test_data).select('unique_id', col('prediction').alias('lr_prediction'))\n",
    "    test_gb_predictions = gbt_model.transform(test_data).select('unique_id', col('prediction').alias('gb_prediction'))\n",
    "\n",
    "    meta_test_data = test_data.join(test_rf_predictions, 'unique_id').join(test_lr_predictions, 'unique_id').join(test_gb_predictions, 'unique_id')\n",
    "\n",
    "    meta_test_data = assembler.transform(meta_test_data)\n",
    "\n",
    "    predictions = meta_lr_model.transform(meta_test_data)\n",
    "\n",
    "    predictions = predictions.withColumn('prediction_label', expr('prediction'))\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "    meta_auc = evaluator.evaluate(predictions)\n",
    "    correct_predictions = predictions.filter(predictions['label'] == predictions['prediction_label']).count()\n",
    "    total_predictions = predictions.count()\n",
    "    meta_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    tp = predictions.filter((predictions['label'] == 1) & (predictions['prediction_label'] == 1)).count()\n",
    "    fp = predictions.filter((predictions['label'] == 0) & (predictions['prediction_label'] == 1)).count()\n",
    "    fn = predictions.filter((predictions['label'] == 1) & (predictions['prediction_label'] == 0)).count()\n",
    "\n",
    "    meta_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    meta_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    meta_f1 = 2 * meta_precision * meta_recall / (meta_precision + meta_recall) if (meta_precision + meta_recall) > 0 else 0\n",
    "    # calculate the\n",
    "    print(f'Meta-Classifier - Accuracy: {meta_accuracy}, Precision: {meta_precision}, Recall: {meta_recall}, F1: {meta_f1}')\n",
    "\n",
    "    return {\n",
    "        'Accuracy': meta_accuracy,\n",
    "        'Precision': meta_precision,\n",
    "        'Recall': meta_recall,\n",
    "        'F1': meta_f1,\n",
    "        'AUC': meta_auc\n",
    "    }, meta_lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_meta_normal, meta_lr_model_normal = train_and_evaluate_stacking_model(train_data, test_data, trained_models_normal[\"RandomForest\"], trained_models_normal[\"LogisticRegression\"], trained_models_normal[\"GBT\"], \"features\", use_class_weights=True)\n",
    "\n",
    "print(results_meta_normal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_resampled_over, meta_lr_model_resampled_over = train_and_evaluate_stacking_model(train_data_over, test_data_over, trained_models_resampled_over[\"RandomForest\"], trained_models_resampled_over[\"LogisticRegression\"], trained_models_resampled_over[\"GBT\"], \"features\")\n",
    "\n",
    "print(results_resampled_over)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with Down Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_resampled_down, meta_lr_model_resampled_down = train_and_evaluate_stacking_model(train_data_down, test_data_down, trained_models_resampled_down[\"RandomForest\"], trained_models_resampled_down[\"LogisticRegression\"], trained_models_resampled_down[\"GBT\"], \"features\")\n",
    "\n",
    "print(results_resampled_down)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Summary \n",
    "\n",
    "\n",
    "| Model                | Sampling Method | Accuracy   | Precision | Recall   | F1     | AUC    |\n",
    "|----------------------|-----------------|------------|-----------|----------|--------|--------|\n",
    "| Random Forest        | Class Weights   |  93.16%    |  83.73%   |  99.96%  | 91.13% | 98.22% |\n",
    "| Gradient Boosting    | Class Weights   |  94.50%    |  86.85%   |  99.40%  | 92.70% | 98.80% |\n",
    "| Logistic Regression  | Class Weights   |  93.73%    |  85.08%   |  99.65%  | 99.79% | 98.25% |\n",
    "| Meta-Classifier      | Class Weights   |  94.87%    |  88.22%   |  98.57%  | 93.11% | 98.34% |\n",
    "|----------------------|-----------------|------------|-----------|----------|--------|--------|\n",
    "| Random Forest        | OverSampling    |  94.79%    |   90.62%  |  99.94%  | 95.05% | 98.15% |\n",
    "| Gradient Boosting    | OverSampling    |  95.69%    |   93.45%  |  98.17%  | 95.80% | 98.76% |\n",
    "| Logistic Regression  | OverSampling    |  95.05%    |   91.29%  |  99.62%  | 95.27% | 98.18% |\n",
    "| Meta-Classifier      | OverSampling    |  95.52%    |   92.23%  |  99.43%  | 95.69% | 98.22% |\n",
    "|----------------------|-----------------|------------|-----------|----------|--------|--------|\n",
    "| Random Forest        | UnderSampling   | 94.57%     |  90.28%   |  99.95%  | 94.87% | 98.18% |\n",
    "| Gradient Boosting    | UnderSampling   | 95.69%     |  92.53%   |  99.45%  | 95.86% | 98.81% |\n",
    "| Logistic Regression  | UnderSampling   | 95.19%     |  91.52%   |  99.66%  | 95.41% | 98.29% |\n",
    "| Meta-Classifier      | UnderSampling   | 95.67%     |  93.53%   |  98.16%  | 95.79% | 98.37% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results shows that Gradient Boosting and the Meta-Classifier consistently performed the best across different sampling methods. The use of class weights, over-sampling, and under-sampling techniques significantly improved the models' performance, particularly in recall and AUC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature importance\n",
    "rf_model = trained_models_normal[\"RandomForest\"]\n",
    "feature_importance = rf_model.featureImportances\n",
    "feature_importance = [(features[i], feature_importance[i]) for i in range(len(features))]\n",
    "feature_importance = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance in feature_importance:\n",
    "    print(f'{feature}: {importance}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance analysis from the Random Forest model reveals that DNS related features are the most significant in predicting phishing URLs. Specifically, num_ip_octets, domain_length, has_ip_address, and num_dns_answers are the top contributors, with importance scores of 0.296, 0.256, 0.199, and 0.136, respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedCap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
